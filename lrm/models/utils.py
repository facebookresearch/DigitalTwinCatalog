# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math
import warnings

import numpy as np

import torch
from torch import nn


def zero_module(module):
    for p in module.parameters():
        nn.init.zeros_(p)
    return module


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


# pyre-fixme[10]: Name `Tensor` is used but not defined.
def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0.0, training: bool = False):
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


def bit_to_int(bitcode):
    batch_size, dim, token_num = bitcode.shape
    bitcode = bitcode.detach().cpu().numpy()
    if dim == 8:
        intcode = np.zeros((batch_size, token_num), dtype=np.uint8)
        dtype = np.uint8
    elif dim == 16:
        intcode = np.zeros((batch_size, token_num), dtype=np.uint16)
        dtype = np.uint16
    elif dim == 32:
        intcode = np.zeros((batch_size, token_num), dtype=np.uint32)
        dtype = np.uint32
    elif dim == 64:
        intcode = np.zeros((batch_size, token_num), dtype=np.uint64)
        dtype = np.uint64
    else:
        raise ValueError("dim must be 8, 16, 32, or 64")

    bitcode[bitcode > 0] = 1
    bitcode[bitcode < 0] = -1
    bitcode = bitcode.astype(dtype)

    for n in range(0, dim):
        intcode += ((bitcode[:, n, :] + 1) // 2) * (2**n)
    return intcode


def int_to_bit(intcode, dim):
    batch_size, token_num = intcode.shape
    bitcode = torch.zeros((batch_size, dim, token_num), dtype=torch.float32)
    bitcode = bitcode.to(device=intcode.device)

    for n in range(0, dim):
        bitcode[:, n, :] = intcode % 2
        intcode = intcode // 2
    bitcode = (2 * bitcode - 1) / (dim**0.5)
    return bitcode


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
        bias=True,
        use_weight_norm=False,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)
        self.drop = nn.Dropout(drop)

        if use_weight_norm:
            self.fc1 = nn.utils.parametrizations.weight_norm(self.fc1)
            self.fc2 = nn.utils.parametrizations.weight_norm(self.fc2)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class AttentionPytorch(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
    ):
        super().__init__()
        self.attn = torch.nn.MultiheadAttention(
            embed_dim=dim, num_heads=num_heads, batch_first=True
        )

    def forward(self, x):
        x = self.attn(x, x, x, need_weights=False)[0]
        return x


class Block(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        drop=0.0,
        attn_drop=0.0,
        drop_path=0.0,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        use_pytorch_attn=False,
        use_weight_norm=False,
        norm_bias=True,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim, bias=norm_bias)
        self.attn = AttentionPytorch(dim, num_heads)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim, bias=norm_bias)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
            use_weight_norm=use_weight_norm,
        )

    def forward(self, x):
        y = self.attn(self.norm1(x))
        x = x + self.drop_path(y)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class PatchEmbedPlucker(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.patch_size = patch_size

        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )
        self.proj_plucker = nn.Conv2d(
            6, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def from_vit(self, patch_embed):
        self.proj.load_state_dict(patch_embed.proj.state_dict())

    def forward(self, x, plucker_rays):
        x = self.proj(x) + self.proj_plucker(plucker_rays)
        x = x.flatten(2).transpose(1, 2)
        return x


def batch_norm_to_group_norm(layer):
    """Iterates over a whole model (or layer of a model) and replaces every batch norm 2D with a group norm

    Args:
        layer: model or one layer of a model like resnet34.layer1 or Sequential(), ...
    """
    GROUP_NORM_LOOKUP = {
        16: 1,  # -> channels per group: 16
        32: 2,  # -> channels per group: 16
        64: 4,  # -> channels per group: 16
        128: 8,  # -> channels per group: 16
        256: 16,  # -> channels per group: 16
        512: 32,  # -> channels per group: 16
        1024: 64,  # -> channels per group: 16
        2048: 128,  # -> channels per group: 16
    }
    for name, _ in layer.named_modules():
        if name:
            try:
                # name might be something like: model.layer1.sequential.0.conv1 --> this wont work. Except this case
                sub_layer = getattr(layer, name)
                if isinstance(sub_layer, torch.nn.BatchNorm2d):
                    num_channels = sub_layer.num_features
                    # first level of current layer or model contains a batch norm --> replacing.
                    layer._modules[name] = torch.nn.GroupNorm(
                        GROUP_NORM_LOOKUP[num_channels], num_channels
                    )
            except AttributeError:
                # go deeper: set name to layer1, getattr will return layer1 --> call this func again
                name = name.split(".")[0]
                sub_layer = getattr(layer, name)
                sub_layer = batch_norm_to_group_norm(sub_layer)

    return layer
